{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow-Keras Loss Functions\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../../images/keras-tensorflow-logo.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Losses Functions**\n",
    "\n",
    "In machine learning, a loss function is used to measure the \"cost\" or degree of fit of a model. Therefore it is important to choose appropriatly the cost function based on the predictive task at hand.\n",
    "\n",
    "<img src=\"../../images/svm.png\" width=\"400\">\n",
    "<center>* Hinge Loss  with different error margins*</center>\n",
    "\n",
    "Every deep learning model is an optimization problem that seeks to minimize a loss function over the training set. The loss score is what is used to backpropagate the error signal throughout the neural network and update the model weights during training. It is therefore a crucial component of the learning process.\n",
    "\n",
    "The chose of loss function should optimize the metric we care about (such as accuracy, residual error).\n",
    "\n",
    "Below are examples of loss functions provided by TensorFlow's loss module and by TF-Keras:\n",
    "\n",
    "TensorFlow loss module **`tf.losses`**\n",
    "    \n",
    "    tf.losses.sigmoid_cross_entropy\n",
    "    tf.losses.softmax_cross_entropy\n",
    "    tf.losses.sparse_softmax_cross_entropy\n",
    "    tf.losses.cosine_distance\n",
    "    tf.losses.hinge_loss\n",
    "    tf.losses.log_loss\n",
    "    tf.losses.mean_squared_error\n",
    "    tf.losses.mean_pairwise_squared_error\n",
    "    \n",
    "TF-Keras loss module **`tf.contrib.keras.losses`**\n",
    "\n",
    "    tf_keras_losses.mean_squared_error\n",
    "    tf_keras_losses.mean_absolute_error\n",
    "    tf_keras_losses.binary_crossentropy\n",
    "    tf_keras_losses.categorical_crossentropy\n",
    "    tf_keras_losses.sparse_categorical_crossentropy\n",
    "    tf_keras_losses.cosine_proximity\n",
    "    tf_keras_losses.hinge\n",
    "    tf_keras_losses.squared_hinge\n",
    "\n",
    "All of the loss functions take a pair of predictions and ground truth labels, from which the loss is computed.\n",
    "\n",
    "$$LossScore = Loss(y_{true}, y_{pred})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_keras = tf.contrib.keras\n",
    "tf_keras_losses = tfKeras.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "The simplest type of classification is the **Binary Classification**. The predictions in this case are binary (1 or 0, true or false). It is applicable to 1-class problems and 2-class problems.\n",
    "\n",
    "For example:\n",
    "- 1-class: Is going to rain tomorrow, yes or no?\n",
    "- 2-class: Is this a picture of a dog or a cat?\n",
    "\n",
    "<img src=\"../../images/dog_cat.png\" width=\"500\">\n",
    "\n",
    "We need to construct a loss function that will penalize prediction errors in this sort of tasks. It is important to note that the 2 classes are dependent and mutually exclusive.\n",
    "An example of such loss function is the **sigmoid cross-entropy loss**.\n",
    "\n",
    "**Sigmoid cross-entropy loss** measures the probability error of the correct class.\n",
    "\n",
    "**Sigmoid function** or logistic function only ouputs a single value (between 0 and 1), which represent the prediction probability of the positive class. It is often used as an activation function with saturation points at both extremes. It is also used for binary classification.\n",
    "\n",
    "<img src=\"../../images/sigmoid.jpg\" width=\"200\">\n",
    "\n",
    "$$sigmoid(z)=\\frac {1}{1+e^{-z_j}}$$\n",
    "\n",
    "**Sigmoid function** can be thought of has a special case of softmax where the number of class equals to 2.  Sigmoid functions takes in a single output neuron and the prediction is defined by an arbitrary threshold (often 0.5).\n",
    "\n",
    "**Cross Entropy Loss** for binary classification is equivalent to the negative log-likelihood of the true labels given a probabilistic classifier’s predictions. It is also refered as **log-loss**.\n",
    "\n",
    "$$sigmoidCrossEntropyLoss(z)=-\\log\\left(\\frac {1}{1+e^{-z_j}}\\right)$$\n",
    "\n",
    "Both core TensorFlow and TF-Keras provide their version of a binary loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your binary model predictions\n",
    "class_score = BinaryClassificationModel(inputs)\n",
    "\n",
    "# TensorFlow Core loss\n",
    "cross_entropy_loss = tf.losses.sigmoid_cross_entropy(binary_labels, class_score)\n",
    " \n",
    "# Or TF-Keras loss\n",
    "cross_entropy_loss = tf_keras_losses.binary_crossentropy(binary_labels, class_score)\n",
    "\n",
    "# Or custom TensorFlow operation\n",
    "class_probability = tf.nn.sigmoid(class_score)\n",
    "cross_entropy_loss = - tf.losses.log_loss(binary_labels, class_probability)\n",
    "\n",
    "# model's loss plus any regularization losses.\n",
    "total_loss = tf.losses.get_total_loss(add_regularization_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification\n",
    "\n",
    "What if we have more than two classes? Then this is a **Mulit-class classification** problem.\n",
    "More specifically we have n discrete classes that are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck or house, but not more than one at the time.\n",
    "\n",
    "<img src=\"../../images/multi-class.png\" width=\"200\">\n",
    "\n",
    "**Softmax Cross-Entropy loss** measures the probability error for discrete multi-classification tasks.\n",
    "\n",
    "A common classfier choice for multi-class task is the **Softmax classifier**.\n",
    "The softmax classifier is a generalization of a binary classifier to multiple classes.\n",
    "The softmax classifier has an interpretable output of normalized class probabilities.\n",
    "\n",
    "**Softmax function**\n",
    "$$softmax(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$$\n",
    "\n",
    "**Softmax function** takes a vector of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum to one. Therefore, it guarantees that the sum of all class probabilities is 1.That's why it's used for multi-class classification because you expect your samples to belong to a single class at the time.\n",
    "\n",
    "**Cross-Entropy Loss**\n",
    "$$CrossEntropyLoss = -\\log\\left(\\frac{e^{z_j}}{\\sum_k e^{z_k}}\\right)$$\n",
    "\n",
    "The Softmax classifier minimizes the **cross-entropy** between the estimated class probabilities and the “true” distribution, which can be a soft class distribution or a one-hot encoding of the target labels.\n",
    "\n",
    "If the true distribution is one-hot encoding then the loss simplifies to the **sparse cross-entropy** because the probability of a given label is considered exclusive.\n",
    "\n",
    "This loss is equivalent to minimizing the KL divergence (distance) between the two distributions.\n",
    "\n",
    "$$H(p,q) = - \\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "Another interpretation is the cross-entropy objective wants the predicted distribution to have all of its mass on the correct label.\n",
    "\n",
    "**Probabilistic interpretation**\n",
    "Softmax classifier gives the probability assigned to the correct label $y_i$ given the image $x_i$ and parameterized by $W$.\n",
    "\n",
    "$$P(y_i \\mid x_i; W) = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j} }$$\n",
    "\n",
    "We are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing **Maximum Likelihood Estimation (MLE)**. With added L2 regularization (which equates to a Gaussian prior over the weight matrix $W$), we are instead performing the **Maximum a posteriori (MAP)** estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your multi-class model predictions\n",
    "class_scores = MultiClassificationModel(inputs) # vector of class scores\n",
    "\n",
    "# TensorFlow Core loss\n",
    "cross_entropy_loss = tf.losses.softmax_cross_entropy(one_hot_labels, class_scores)\n",
    "\n",
    "# sparse implementation\n",
    "cross_entropy_loss = tf.losses.sparse_softmax_cross_entropy(one_hot_labels, class_scores)\n",
    "\n",
    "# TF-Keras loss\n",
    "cross_entropy_loss = tf_keras_losses.categorical_crossentropy(one_hot_labels, class_scores)\n",
    "\n",
    "# sparse implementation\n",
    "cross_entropy_loss = tf_keras_losses.sparse_categorical_crossentropy(one_hot_labels, class_scores)\n",
    "\n",
    "# Or custom TensorFlow operation\n",
    "class_probabilities = tf.nn.softmax(class_scores)\n",
    "cross_entropy_loss = tf.losses.log_loss(one_hot_labels, class_probabilities)\n",
    "\n",
    "# model's loss plus any regularization losses.\n",
    "total_loss = tf.losses.get_total_loss(add_regularization_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Label Classification\n",
    "\n",
    "**Multi-Label Classification** is when a sample observation can belong to multiple classes at the same time.\n",
    "\n",
    "<img src=\"../../images/multi-label.jpeg\" width=\"700\">\n",
    "\n",
    "\n",
    "We can rephrase multi-label learning as the problem of finding a model that maps inputs x to binary vectors y, rather than scalar outputs as in the ordinary classification problem.\n",
    "With this interpretation, the solution is to apply an independent sigmoid function for each label.\n",
    "\n",
    "Sigmoid functions give predictions independent of all other classes. We lose the probabilistic interpretation because the prediction sum can range anywhere from 0 to K.\n",
    "\n",
    "**Multi-Label Sigmoid Cross-Entropy** measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your multi-label model predictions\n",
    "class_scores = MultiLabelClassificationModel(inputs) # vector of class scores\n",
    "\n",
    "# TensorFlow core loss\n",
    "loss = tf.losses.sigmoid_cross_entropy(multi_class_labels, class_scores)\n",
    "\n",
    "# model's loss plus any regularization losses.\n",
    "total_loss = tf.losses.get_total_loss(add_regularization_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss\n",
    "**Hinge Loss** is used in Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\\Delta$.\n",
    "\n",
    "<img src=\"../../images/hinge2.png\" width=\"500\">\n",
    "\n",
    "For the score function $s_j = f(x_i, W)_j$. The Multiclass SVM loss for the i-th example is then formalized as follows:\n",
    "\n",
    "$$HingeLoss = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n",
    "\n",
    "**Squared Hinge Loss** SVM (or L2-SVM), which simply squares the margin error. It penalizes the violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.\n",
    "\n",
    "$$SquaredHingeLoss = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your multi-class model predictions\n",
    "class_scores = MultiClassificationModel(inputs) # vector of class scores\n",
    "\n",
    "# TensorFlow core loss\n",
    "loss = tf.losses.hinge_loss(one_hot_labels, class_scores)\n",
    "\n",
    "# TF-Keras loss\n",
    "loss = tf_keras_losses.hinge(one_hot_labels, class_scores)\n",
    "\n",
    "# TF-Keras square hinge loss\n",
    "loss = tf_keras_losses.squared_hinge(one_hot_labels, class_scores)\n",
    "\n",
    "# model's loss plus any regularization losses.\n",
    "total_loss = tf.losses.get_total_loss(add_regularization_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression Loss\n",
    "Not all predictive tasks involve classification with distinct labels. Sometimes we need to predict a continuous variable. For example the price of a house given a set of features, or predict how much snow will fall given weather data.\n",
    "\n",
    "For these tasks we need to look at a different class of losses called **Regression Losses**.\n",
    "\n",
    "<img src=\"../../images/regression.png\" width=\"700\">\n",
    "\n",
    "\n",
    "**Mean squared error (MSE)** measures the average of the squares of the errors or deviations. In the context of regression analysis, it measures the quality of an estimator—it is always non-negative, and values closer to zero are better.\n",
    "\n",
    "$$\\operatorname {MSE}={\\frac  {1}{n}}\\sum _{{i=1}}^{n}({\\hat  {Y_{i}}}-Y_{i})^{2}$$\n",
    "\n",
    "**Mean absolute error (MAE)** measures the average absolute errors. This loss is used to measure how close forecasts or predictions are to the eventual outcomes.\n",
    "\n",
    "$$\\operatorname {MAE}={\\frac  {1}{n}}\\sum _{{i=1}}^{n}|{\\hat  {Y_{i}}}-Y_{i}| $$\n",
    "\n",
    "**Mean pairwise squared error(MPSE)** Unlike `mean_squared_error`, which is a measure of the differences between\n",
    "corresponding elements of `predictions` and `labels`,\n",
    "`mean_pairwise_squared_error` is a measure of the differences between pairs of\n",
    "corresponding elements of `predictions` and `labels`.\n",
    "\n",
    "For example:\n",
    "if `labels = [a, b, c]` and `predictions = [x, y, z]`, there are\n",
    "three pairs of differences are summed to compute the loss:\n",
    " \n",
    "$$\\operatorname {MPSE} = \\frac{((a-b) - (x-y))^2 + ((a-c) - (x-z))^2 + ((b-c) - (y-z))^2}{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your multi-class model predictions\n",
    "y_pred = RegressionModel(inputs)\n",
    "\n",
    "# TensorFlow core loss\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# TF-Keras loss\n",
    "loss = tf_keras_losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Mean absolute error\n",
    "loss = tf_keras_losses.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Main pairwise squared error\n",
    "loss = tf.losses.mean_pairwise_squared_error(y_true, y_pred)\n",
    "\n",
    "# model's loss plus any regularization losses.\n",
    "total_loss = tf.losses.get_total_loss(add_regularization_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "### TensorFlow Evaluation Metrics\n",
    "-  Explore evaluation metrics provided by TensorFlow core and TF-Keras\n",
    "\n",
    "<img src=\"../../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TF-1_1]",
   "language": "python",
   "name": "conda-env-TF-1_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
