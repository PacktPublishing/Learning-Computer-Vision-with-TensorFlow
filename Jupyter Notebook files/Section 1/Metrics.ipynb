{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow-Keras Evaluation Metrics\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../../images/keras-tensorflow-logo.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training a Deep Learning model inspecting the loss alone does not provide interpretable measure about how well the neural network is training. \n",
    "\n",
    "Instead we compute at regular intervals evaluations metrics to score the model performance based on the task we care about. \n",
    "\n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "- a metric is a performance measure\n",
    "- a metric is not a loss function (losses are directly optimized during training)\n",
    "- a metric is consistent with the task of the problem\n",
    "- for example, we may want to minimize cross-entropy, but our metrics of interest might be accuracy or precision score\n",
    "- a metric is not necessarily differentiable, and therefore cannot be used as a loss\n",
    "\n",
    "**Computing model performance includes**\n",
    "\n",
    "- loading the (subset) data\n",
    "- performing inference\n",
    "- comparing the results to the ground truth\n",
    "- recording the evaluation scores\n",
    "- repeating periodically\n",
    "\n",
    "Below are a summary of the most common evaluation metrics:\n",
    "\n",
    "<img src=\"../../images/metrics.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are examples of evaluation metrics provided by TensorFlow's metrics module and by TF-Keras:\n",
    "\n",
    "TensorFlow metrics module **`tf.metrics`**\n",
    "\n",
    "    tf.metrics.accuracy\n",
    "    tf.metrics.auc\n",
    "    tf.metrics.precision\n",
    "    tf.metrics.recall\n",
    "    tf.metrics.recall_at_k\n",
    "    tf.metrics.true_positives\n",
    "    tf.metrics.false_negatives\n",
    "    tf.metrics.false_positives\n",
    "    tf.metrics.mean_per_class_accuracy\n",
    "    tf.metrics.precision_at_thresholds\n",
    "    tf.metrics.recall_at_thresholds\n",
    "    \n",
    "TF-Keras metrics module **`tf.contrib.keras.metrics`**\n",
    "\n",
    "    tf_keras_metrics.binary_accuracy\n",
    "    tf_keras_metrics.categorical_accuracy\n",
    "    tf_keras_metrics.top_k_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_keras = tf.contrib.keras\n",
    "tf_keras_metrics = tfKeras.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Precision\n",
    "\n",
    "**Precision** is a description of **random errors**, a measure of **statistical variability**. In other words, the closeness of two or more measurements to each other.\n",
    "\n",
    "**Accuracy** is a description of **systematic errors**, a measure of **statistical bias**. In other words, the closeness of a measured value to the true value.\n",
    "\n",
    "<img src=\"../../images/accurate.png\" width=\"700\">\n",
    "\n",
    "**Precision and Accuracy** are related to the **Bias-variance trade-off** found in Machine Learning models. \n",
    "\n",
    "<img src=\"../../images/ModelError.png\" width=\"400\">\n",
    "\n",
    "\n",
    "**High variance** can be caused by the model **overfitting** to the training set. The performance may be great on the training set, but the model can't generalize to out of sample data and it's performance will greatly vary across different datasets (low precision)\n",
    "\n",
    "*Solution:*\n",
    "- add regularization to the model\n",
    "- collect more data\n",
    "- decrease model expressiveness (complexity)\n",
    "- bagging (Bootstrap Aggregating) or other resampling techniques (random forest) \n",
    "\n",
    "**High bias** can be caused by **under-fitting** the model. The model could give you consicent predictions but the performance will be systematically low (low accuracy)\n",
    "\n",
    "*Solution:*\n",
    "- increase model expressiveness (complexity)\n",
    "- collect more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "# TensorFlow core accuracy metric\n",
    "accuracy = tf.metrics.accuracy(labels, predictions)\n",
    "\n",
    "# TF-Keras accuracy metrics\n",
    "accuracy = tf_keras_metrics.binary_accuracy(labels, predictions)\n",
    "\n",
    "# multi-class accuracy\n",
    "accuracy = tf_keras_metrics.categorical_accuracy(labels, predictions)\n",
    "\n",
    "# Top-K accuracy\n",
    "accuracy = tf_keras_metrics.top_k_categorical_accuracy(labels, predictions)\n",
    "\n",
    "# mean per class accuracy\n",
    "accuracy = tf.metrics.mean_per_class_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall and F-1 Score\n",
    "\n",
    "<img src=\"../../images/recall.png\" width=\"200\">\n",
    "\n",
    "$$Recall = \\frac{TruePositive}{PositiveSamples} = \\frac{TruePositive}{TruePositive + FalseNegative} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "# TensorFlow core recall metric\n",
    "recall = tf.metrics.recall(labels, predictions)\n",
    "\n",
    "# Recall at k\n",
    "recall = tf.metrics.recall_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluation Metrics\n",
    "\n",
    "**F1 score is the harmonic mean of precision and recall**\n",
    "\n",
    "$${\\displaystyle F_{1}=2\\cdot {\\frac {1}{{\\tfrac {1}{\\mathrm {recall} }}+{\\tfrac {1}{\\mathrm {precision} }}}}=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1 score metric\n",
    "def F1_score(labels, predictions):\n",
    "    precision = tf.metrics.precision(labels, predictions)\n",
    "    recall = tf.metrics.recall(labels, predictions)\n",
    "    return 2 * tf.multiply(precision, recall) / tf.add(precision, recall)\n",
    "\n",
    "# recall metric\n",
    "def recall(labels, predictions):\n",
    "    TP = tf.metrics.true_positives(labels, predictions)\n",
    "    FN = tf.metrics.false_negatives(labels, predictions)\n",
    "    return TP / tf.add(TP,FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ROC AUC\n",
    "An **ROC curve** is the most commonly used way to visualize the performance of a **binary classifier**, and **AUC** is a good way summarize the **classifier's performance** in a single number. This number is between 0.5 and 1. \n",
    "\n",
    "A good classifier is a classifier that is able to separate the classes in such a way that the correct prediction can be made.\n",
    "\n",
    "<img src=\"../../images/Roc1.png\" width=\"300\">\n",
    "\n",
    "**ROC curve**, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the **true positive rate (TPR**) against the **false positive rate (FPR)** at various threshold settings. This is in comparaisn to misclassification rate such as accuracy that only represents your error rate for a single threshold. \n",
    "\n",
    "the **Area Under the Curve (AUC)** is literally just the percentage of the box that is under the curve. This metric quantifies the performance of a classifier into one number for model comparaison.\n",
    "\n",
    "You can think of AUC as representing the **probability that a classifier will rank a randomly chosen positive observation higher than a randomly chosen negative observation**, and thus it is a useful metric even for datasets with **highly unbalanced classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "# TensorFlow core AUC metric\n",
    "auc = tf.metrics.auc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Streaming Metrics\n",
    "\n",
    "Each metric function adds nodes to the graph that hold the state necessary to compute the value of the metric as well as a set of operations that actually perform the computation. \n",
    "\n",
    "Every TensorFlow metrics can be estimated over streamming data and computed on dynamically valued Tensors, as sample batches are evaluated.\n",
    "\n",
    "Every metric evaluation is composed of three steps:\n",
    "\n",
    "1. **Initialization** - initialize the variables used to compute the metrics.\n",
    "2. **Aggregation** - updating the values of the metric state. (**update_op**)\n",
    "3. **Finalization** - computing the final metric value. (**value_tensor**)\n",
    "\n",
    "\n",
    "# Defining Multiple Metrics\n",
    "\n",
    "In practice, we commonly want to evaluate multiple metrics at the same time. Below is how you would define three different metrics. Each metric generate it's own update operation that accumulates the results across multiple batches.\n",
    "\n",
    "\n",
    "**Dictionary Aggregation**\n",
    "\n",
    "We can aggregate metrics into a dictionary and give each one of them names. In practice, we commonly want to evaluate across many batches and multiple metrics. This is done by running the aggregate metric computation operations multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model predictions\n",
    "predictions = DeepLearningModel(inputs)\n",
    "\n",
    "\n",
    "# Aggregates the value and update ops into dictionary:\n",
    "names_to_values, names_to_updates = tf.contrib.slim.metrics.aggregate_metric_map({\n",
    "    'eval/Accuracy': tf.metrics.accuracy(labels, predictions),\n",
    "    'eval/Precision': tf.metrics.precision(labels, predictions),\n",
    "    'eval/Recall': tf.metrics.recall(labels, predictions)\n",
    "})\n",
    "\n",
    "\n",
    "# Evaluate the model using 1000 batches of data:\n",
    "num_batches = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    # run metrics over multiple batches\n",
    "    for batch_id in range(num_batches):\n",
    "        sess.run(names_to_updates.values())\n",
    "\n",
    "    # Get each metric end value\n",
    "    metric_values = sess.run(name_to_values.values())\n",
    "    for metric, value in zip(names_to_values.keys(), metric_values):\n",
    "        print('Metric %s has value: %f' % (metric, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "### Tensorflow Optimizers\n",
    "-  Why an optimizer is needed deep neural networks and what are the different types.\n",
    "\n",
    "<img src=\"../../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TF-1_1]",
   "language": "python",
   "name": "conda-env-TF-1_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
